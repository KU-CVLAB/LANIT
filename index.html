<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>LANIT</title>
	<meta property="og:image" content="https://KU-CVLAB.github.io/LANIT/resources/teaser_lanit.png"/>
	<meta property="og:title" content="LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data" />
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data (CVPR 2023)</span>
		<table align=center width=1100px>
			<table align=center width=1100px>
				<tr>
					<td align=center width=180px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.co.kr/citations?hl=ko&user=Fg0DBKwAAAAJ">Jihye Park<sup>*1</sup></a></span>
						</center>
					</td>
					<td align=center width=220px>
						<center>
							<span style="font-size:24px"><a href="https://github.com/sunwoo76">Sunwoo Kim<sup>*1</sup></a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.co.kr/citations?user=oDplwx8AAAAJ&hl=ko&oi=sra">Soohyun Kim<sup>*1</sup></a></span>
						</center>
					</td>
					<td align=center width=180px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.co.kr/citations?user=7NBlQw4AAAAJ&hl=ko&oi=ao">Jaejun Yoo<sup>†2</sup></a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.co.kr/citations?user=BWBGrEEAAAAJ&hl=ko&oi=ao">Youngjung Uh<sup>†3</sup></a></span>
						</center>
					</td>
					<td align=center width=250px>
						<center>
							<span style="font-size:24px"><a href="https://cvlab.korea.ac.kr/members">Seungryong Kim<sup>†1</sup></a></span>
						</center>
					</td>
				</tr>
			</table>
						
			<table align=center width=800px>
				<tr>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px">Korea University<sup>1</sup></a></span>
						</center>
					</td>
			
					<td align=center width=150px>
						<center>
							<span style="font-size:24px">UNIST<sup>2</sup></a></span>
						</center>
					</td>
	
					<td align=center width=150px>
						<center>
							<span style="font-size:24px">Yonsei University<sup>3</sup></a></span>
						</center>
					</td>


				</tr>
			</table>
			<small>
							    * Equal contribution    † Corresponding Author
						
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2208.14889'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/KU-CVLAB/LANIT'>[GitHub]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>
	<br>
	
	<center>
		<table align=center width=850px>
			<tr>
				<td width=420px>
					<center>
						<img class="round" style="width:800px" src="./resources/teaser_lanit.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					LANIT firstly highlights to address multiple attributes in one sample for Image-to-Image translation tasks.
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Existing techniques for image-to-image translation commonly have suffered from two critical problems: heavy reliance on per-sample domain annotation and/or inability of handling multiple attributes per image. Recent methods adopt clustering approaches to easily provide per-sample annotations in an unsupervised manner.
				However, they cannot account for the real-world setting; one sample may have multiple attributes.
				In addition, the semantics of the clusters are not easily coupled to human understanding.
				To overcome these, we present a LANguage-driven Image-to-image Translation model, dubbed LANIT.
				We leverage easy-to-obtain candidate domain annotations given in texts for a dataset and jointly optimize them during training.
				The target style is specified by aggregating multi-domain style vectors according to the multi-hot domain assignments.
				As the initial candidate domain texts might be inaccurate, we set the candidate domain texts to be learnable and jointly fine-tune them during training.
				Furthermore, we introduce a slack domain to cover samples that are not covered by the candidate domains.
				Experiments on several standard benchmarks demonstrate that LANIT achieves comparable or superior performance to existing models
			</td>
		</tr>
	</table>
	<br>
	<hr>

	<center><h1> Levels of Supervision </h1></center>
	
	<table align=center width=500px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/level_sup.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					For unpaired image-to-image translation, (a) conventional methods (CycleGAN, MUNIT, FUNIT, StarGAN, SEMIT) require at least per-sample-level domain supervision, which is often hard to collect.
					To overcome this, (b) unsupervised learning methods (TUNIT, Style aware discriminator) learn image translation model using a dataset itself without any supervision, but it shows limited performance and lacks the semantic understanding of each cluster, limiting its applicability.
					Unlike them, (c) we present a novel framework for image translation that requires a dataset with possible textual domain descriptions (i.e., dataset-level annotation), which achieves comparable or even better performance than previous methods.
				</td>
			</tr>
		</center>
	</table>
	<br>
	<hr>

	<center><h1> Network Configuration </h1></center>
	
	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/network_confing_lanit2.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					We propose a language-driven Image-to-Image translation framework with candidate "dataset-level" domain annotations, which is more practical than fully-unsupervised methods in the real-world.
				</td>
			</tr>
		</center>
	</table>
	<br>
	<hr>
	
	<center><h1> Qualitative Results </h1></center>
	
	<h2> Controlling the number of attributes to translate </h2>
	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:820px" src="./resources/celeb_control.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	
	<h2> Latent-guided translation </h2>
	<h3> AnimalFaces-10 </h3>
	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/animal_lat.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	
	<h3> Food-10 </h3>
	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/food_lat.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	
	<h2> Reference-guided translation </h2>
	<h3> LSUN-Church </h3>
	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/lsunchurch.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	
	<h3> LSUN-Car </h3>
	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/lsuncar.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	
	<h3> MetFace </h3>
	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/metface.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	
	<h3> Anime </h3>
	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/anime.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<br>
	<hr>
	
	<table align=center width=450px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href="https://arxiv.org/abs/2208.14889"><img class="layered-paper-big" style="height:175px" src="./resources/paper_lanit.png"/></a></td>
			<td><span style="font-size:14pt">J. Park, S. Kim, S. Kim, J. Yoo, Y. Uh, S. Kim<br>
				<b>LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data.</b><br>
				(hosted on <a href="https://arxiv.org/abs/2208.14889">ArXiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

